---
title: "The 2024 U.S. Presidential Election Forecast"
subtitle: "Summary of technical model details"
author: Haowei Fan, Fangning Zhang, Shaotong Li
date: 13/10/2024
contact: haowei.fan@mail.utoronto.ca
licence: MIT
date-format: long
format: pdf
---

```{r}
#| include: false
#| warning: false
#| message: false
library(tidyverse)
library(knitr)
library(dplyr)
library(ggplot2)
library(patchwork)
library(gridExtra)
library(arrow)
library(knitr)  
library(kableExtra) 
```

# Overview

The purpose of this document is to explain the details used in the linear model.This linear model is used to forecast the U.S. 2024 president election by predicting the wining rate outcomes of polls based on several features of polls. Broadly, the model can be described as a Linear hierachical model:

1. A model to capture the association between the outcome (the support or winning rate of candidate) and the input variables (poll reliability score, quality-related factors, and etc)

2.Each independent variables in this model is consider distributed normally, so as the intercept.

## Overview of the input values for prediction

The values are selected based on the significant value of variables of each candidate.The significant values of variables are chosen based on the their distribution.

For numeric variables, we chose either the mean or the median as the representative feature value, depending on the data's distribution.
If the data distribution was approximately symmetric (low skewness), we used the mean, as it best represents the central tendency。

For highly skewed data, we used the median to avoid the influence of outliers, providing a more robust measure of central tendency.

For categorical variables, we selected the mode (the most frequent category) as the feature value.
The mode is often the best representative of categorical data, as it reflects the most common category and thus the main trend within the data.

For Boolean variables, we identified whether TRUE or FALSE was more frequent and used the most common value as the feature value.
This approach ensures that the feature value represents the predominant condition in the data, rather than relying on a percentage or proportion.

Here are the chosen significant values for Trump and Harris:

```{r}
#| echo: false
#| warning: false
#| message: false

# Load the Trump and Harris linear models
trump_linear_model <- readRDS(here::here("models", "Trump_model.rds"))
harris_linear_model <- readRDS(here::here("models", "Harris_model.rds"))

# Define the modified candidate data
candidate_data <- tibble(
  numeric_grade = c(2.15, 2.20),
  pollscore = c(-0.368, -0.4),
  methodology = c("level3", "level3"),
  transparency_score = c(6.17, 6.37),
  sample_size = c(1003, 1000),
  ranked_choice_reallocated = c(FALSE, FALSE),
  hypothetical = c(TRUE, FALSE),
  score = c(45.0, 46.8),
  duration = c(3, 3)
)

# Ensure population is a factor if it’s needed in the model (use any appropriate levels)
candidate_data$population <- factor(c("rv", "lv"), levels = c("rv", "lv"))

# Make predictions for Trump and Harris using the modified candidate_data
candidate_data$Trump_Predicted_Support <- predict(trump_linear_model, newdata = candidate_data)
candidate_data$Harris_Predicted_Support <- predict(harris_linear_model, newdata = candidate_data)

# Create a combined data frame for plotting purposes
predictions_df <- data.frame(
  Candidate = c("Donald Trump", "Kamala Harris"),
  Predicted_Support = c(mean(candidate_data$Trump_Predicted_Support, na.rm = TRUE), 
                        mean(candidate_data$Harris_Predicted_Support, na.rm = TRUE))
)

# Remove the prediction and score columns from the candidate data
candidate_data <- candidate_data %>%
  select(-Trump_Predicted_Support, -Harris_Predicted_Support, -score)

# Convert all remaining columns to character for compatibility in pivot_longer
candidate_data <- candidate_data %>% mutate(across(everything(), as.character))

# Transpose the data for a two-column format
feature_values_table <- candidate_data %>%
  pivot_longer(cols = everything(), names_to = "Variable", values_to = "Value") %>%
  mutate(Candidate = rep(c("Donald Trump", "Kamala Harris"), each = ncol(candidate_data))) %>%
  select(Candidate, Variable, Value) %>%
  pivot_wider(names_from = "Candidate", values_from = "Value")  # Reshape into two columns

# Display the feature values table in a two-column, aesthetic format
kable(feature_values_table, align = c("l", "c", "c"), caption = "Candidate Feature Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), full_width = FALSE) %>%
  column_spec(1, bold = TRUE, width = "6cm") %>%   # Widen the Variable column
  column_spec(2, width = "5cm") %>%                # Increase width of Trump column
  column_spec(3, width = "5cm") %>%                # Increase width of Harris column
  add_header_above(c(" " = 1, "Candidate Feature Summary" = 2))
```

# Variables explained and outcomes estimated

These are the independent variables used in the model:

```{=tex}
\begin{itemize}
\item $\alpha_i$: Poll reliability score
\item $\beta_i$: Poll quality score (numeric grade)
\item $\gamma_i$: Poll transparency score
\item $\delta_i$: Poll duration
\item $\theta_i$: Sample size
\item $\kappa_i$: Population type surveyed
\item $\lambda_i$: Indicator of whether the poll is hypothetical
\item $\mu_i$: Poll methodology type
\item $\rho_i$: Rank-choice reallocation usage
\end{itemize}
```

This is the dependent variable, or the outcome of this model:

```{=tex}
$y_i$ : Winning rate (percentage) for a candidate in the 2024 U.S. presidential election polls
```


# Model

```{=tex}
\begin{align} 
y_i | \mu_i, \sigma &\sim \mbox{Normal}(\mu_i, \sigma) \\
\mu_i &= \alpha + \beta_1 \times \phi_i + \beta_2 \times \beta_i + \beta_3 \times \gamma_i + \beta_4 \times \delta_i \\
      &\quad + \beta_5 \times \theta_i + \beta_6 \times \kappa_i + \beta_7 \times \lambda_i + \beta_8 \times \mu_i + \beta_9 \times \rho_i \\
\alpha &\sim \mbox{Normal}(0, 10) \\
\beta_j &\sim \mbox{Normal}(0, 2.5) \quad \text{for each } j = 1, \dots, 9 \\
\sigma &\sim \mbox{Exponential}(1)
\end{align}
```

This model combines a normal likelihood with hierarchical priors, creating a Bayesian linear regression framework. The priors on the coefficients and intercept provide regularization, helping to avoid overfitting by constraining the values of $α_i$ and $β_j$ coefficients. The prior on $σ$ helps control the model's uncertainty about the data by penalizing very high variance. This setup allows for flexibility in estimating $μ_i$ while accounting for the effects of the predictors and controlling for the noise in the data.

## Hirerachical structure

The hierarchical structure in this model is established through the use of priors on both the intercept $α$ and the coefficients $β_j$, which represent the relationship between predictors and the outcome variable $y_i$. Each coefficient $β_j$ has a normal prior centered around zero with a moderate spread (standard deviation of 2.5), allowing for some variability in the effect of each predictor while constraining extreme values. The intercept $α$ also has a prior centered around zero with a larger standard deviation (10), reflecting greater uncertainty in its baseline effect. Additionally, the standard deviation $σ$, which governs the noise in the data, is given an exponential prior, favoring smaller values to encourage a tighter fit. This hierarchical structure allows the model to borrow strength across predictors, promoting more stable estimates and helping to regularize the model by introducing prior beliefs about the distribution of parameters, thereby preventing overfitting.

## Steps of prediction

### Load Models and Data
The linear models for both Trump and Harris were loaded using readRDS(). We use ths featured value mentioned before as input values in our model.

### Data Preprocessing
The hypothetical column was converted to logical data type where necessary, ensuring data compatibility with prediction models.

### Prediction and Visualization
```{r}
#| echo: false
#| warning: false
#| message: false
# Adjust the bar plot height
bar_plot <- ggplot(predictions_df, aes(x = Candidate, y = Predicted_Support, fill = Candidate)) +
  geom_bar(stat = "identity", width = 0.6, color = "white") +
  geom_text(aes(label = paste0(round(Predicted_Support, 1), "%")), 
            vjust = -0.5, size = 5) +
  labs(title = "",
       x = "Candidate",
       y = "Predicted Support (%)") +
  theme_minimal() +
  scale_fill_manual(values = c("Donald Trump" = "skyblue", "Kamala Harris" = "salmon")) +
  ylim(0, max(predictions_df$Predicted_Support) * 1.2)  # Set upper limit to make space for labels

# Print the bar plot
print(bar_plot)

# Define data for the pie chart, including "Other Candidates" to fill up to 100%
pie_data <- data.frame(
  Candidate = c("Donald Trump", "Kamala Harris", "Other Candidates"),
  Support = c(45.2, 45.9, 100 - (45.2 + 45.9))
)

# Create the pie chart
pie_chart <- ggplot(pie_data, aes(x = "", y = Support, fill = Candidate)) +
  geom_bar(stat = "identity", width = 1, color = "white") +
  coord_polar("y", start = 0) +
  labs(title = "") +
  theme_void() +  # Remove background and axis
  theme(legend.position = "right") +
  scale_fill_manual(values = c("Donald Trump" = "skyblue", "Kamala Harris" = "salmon", "Other Candidates" = "grey")) +
  geom_text(aes(label = ifelse(Support > 5, paste0(round(Support, 1), "%"), "")), 
            position = position_stack(vjust = 0.5), color = "white", size = 5)

# Print the pie chart
print(pie_chart)


```





